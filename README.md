# HCFN
## Adaptive Fuzzy-Convolution and TSK-Guided Attention for Interpretable EEG MI Decoding
### Abstract
Brain-computer Interface (BCI) technology enables direct communication between the brain and external devices via non-invasive methods and holds significant potential in neuroengineering, rehabilitation, and human-computer interaction. However, decoding motor imagery (MI) from electroencephalogram (EEG) signals remains challenging due to these signals’ non-stationary characteristics and the limited interpretability of existing deep learning models. In this paper, we propose a novel Hierarchical Collaborative Fuzzy Network (HCFN) for interpretable EEG‑based MI decoding. We introduce an Adaptive Fuzzy Temporal Convolutional Network (AFTCN) that employs dynamic fuzzy kernels within causal convolutions to extract robust temporal features from EEG signals. Additionally, we design a fuzzy attention-guided Takagi-Sugeno-Kang (TSK) architecture that achieves a tighter integration between feature extraction and fuzzy inference through a novel fuzzy feedback loop, thereby improving the discriminability of extracted features. Extensive experiments on the BCI Competition IV-2a, IV-2b and OpenBMI datasets, under both subject-dependent and cross-subject evaluation paradigms, demonstrate that the proposed model outperforms state-of-the-art methods in classification accuracy and Cohen’s kappa. Furthermore, we provide multi-level interpretability analyses, from macro to micro perspectives, elucidating the model’s decision-making processes and highlighting the advantages of our collaborative reasoning framework over conventional cascaded approaches.

![image](Structure.png)
